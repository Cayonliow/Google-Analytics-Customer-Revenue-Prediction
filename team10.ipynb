{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-262b91b07f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchained_assignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d448c836b5fc2d42062f74d6fd6c70833c872de"
   },
   "source": [
    "**About the dataset:**\n",
    "\n",
    "Similar to most other kaggle competitions, we are given two datasets\n",
    "\n",
    "train_v2.csv\n",
    "test_v2.csv\n",
    "Each row in the dataset is one visit to the store. We are predicting the natural log of the sum of all transactions per user.\n",
    "\n",
    "The data fields in the given files are\n",
    "\n",
    "fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n",
    "channelGrouping - The channel via which the user came to the Store.\n",
    "date - The date on which the user visited the Store.\n",
    "device - The specifications for the device used to access the Store.\n",
    "geoNetwork - This section contains information about the geography of the user.\n",
    "sessionId - A unique identifier for this visit to the store.\n",
    "socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n",
    "totals - This section contains aggregate values across the session.\n",
    "trafficSource - This section contains information about the Traffic Source from which the session originated.\n",
    "visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n",
    "visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n",
    "visitStartTime - The timestamp (expressed as POSIX time).\n",
    "Also it is important to note that some of the fields are in json format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def load_df(csv_path='../data/train10000.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9d2191d95f1e833aabb6028e27845d08412335e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = load_df()\n",
    "test_df = load_df(\"../data/test10000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "d43c76b6012564b4ee0e0dee77f97ceb9fe6dc83"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "096eb05e99e772744ac2f6b18a0e42f8cea23476"
   },
   "outputs": [],
   "source": [
    "#train_df[\"totals.transactionRevenue\"] = [0] * 99\n",
    "train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\n",
    "gdf = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('TransactionRevenue', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49b0144e32a90c436d7a2bbe449be478911fcdc6"
   },
   "source": [
    "**Target Variable Exploration:**\n",
    "\n",
    "Since we are predicting the natural log of sum of all transactions of the user, let us sum up the transaction revenue at user level and take a log and then do a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "901ed77346f636a359eeba093e4567f5bda77aca"
   },
   "outputs": [],
   "source": [
    "nzi = pd.notnull(train_df[\"totals.transactionRevenue\"]).sum()\n",
    "nzr = (gdf[\"totals.transactionRevenue\"]>0).sum()\n",
    "print(\"Number of instances in train set with non-zero revenue : \", nzi, \" and ratio is : \", nzi / train_df.shape[0])\n",
    "print(\"Number of unique customers with non-zero revenue : \", nzr, \"and the ratio is : \", nzr / gdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "430ab11f641f56de087de3a26abe9c8893c4d9a0"
   },
   "source": [
    "So the ratio of revenue generating customers to customers with no revenue is in the ratio os 1.3%\n",
    "\n",
    "Since most of the rows have non-zero revenues, in the following plots let us have a look at the count of each category of the variable along with the number of instances where the revenue is not zero.\n",
    "\n",
    "Number of visitors and common visitors:\n",
    "\n",
    "Now let us look at the number of unique visitors in the train and test set and also the number of common visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6487bbe7550f307b2a0dfd8acf59c2f6888a5dda"
   },
   "outputs": [],
   "source": [
    "const_cols = [c for c in train_df.columns if train_df[c].nunique(dropna=False)==1 ]\n",
    "const_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba55c91b03e3b0523dc7cd62e42fc66c785ef6a4"
   },
   "source": [
    "Baseline Model:\n",
    "\n",
    "Now let us build a baseline model on this dataset. Before we start building models, let us look at the variable names which are there in train dataset and not in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2b14b862fb8610b3895e9a88f60c9603b8cbe38"
   },
   "outputs": [],
   "source": [
    "print(\"Variables not in test but in train : \", set(train_df.columns).difference(set(test_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05a5d9101a3ebbbb868e7e84a7f3dda73ed05487"
   },
   "source": [
    "So apart from target variable, there is one more variable \"trafficSource.campaignCode\" not present in test dataset. So we need to remove this variable while building models. Also we can drop the constant variables which we got earlier.\n",
    "\n",
    "Also we can remove the \"sessionId\" as it is a unique identifier of the visit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7b20a138b28262f61d5705994271de65b38a088"
   },
   "outputs": [],
   "source": [
    "cols_to_drop = const_cols \n",
    "\n",
    "train_df = train_df.drop(cols_to_drop , axis=1)\n",
    "test_df = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2449c24335647a36e26a00c2b0dd197af76bf881"
   },
   "source": [
    "Now let us create development and validation splits based on time to build the model. We can take the last two months as validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52082fe0d685caaa5793455d73fe127be497fe4d"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def scatter_plot(cnt_srs, color):\n",
    "    trace = go.Scatter(\n",
    "        x=cnt_srs.index[::-1],\n",
    "        y=cnt_srs.values[::-1],\n",
    "        showlegend=False,\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "train_df['date'] = train_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\n",
    "cnt_srs = train_df.groupby('date')['totals.transactionRevenue'].agg(['size', 'count'])\n",
    "cnt_srs.columns = [\"count\", \"count of non-zero revenue\"]\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "#cnt_srs.index = cnt_srs.index.astype('str')\n",
    "trace1 = scatter_plot(cnt_srs[\"count\"], 'red')\n",
    "trace2 = scatter_plot(cnt_srs[\"count of non-zero revenue\"], 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "049c912481e83b2abc933ae178716f50260327fc"
   },
   "outputs": [],
   "source": [
    "# Impute 0 for missing target values\n",
    "train_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\n",
    "train_y = train_df[\"totals.transactionRevenue\"].values\n",
    "train_id = train_df[\"fullVisitorId\"].values\n",
    "test_id = test_df[\"fullVisitorId\"].values\n",
    "\n",
    "\n",
    "# label encode the categorical variables and convert the numerical variables to float\n",
    "cat_cols = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.country\", \"geoNetwork.metro\",\n",
    "            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n",
    "            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n",
    "            \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "            \"trafficSource.adwordsClickInfo.gclId\", \n",
    "            \"trafficSource.adwordsClickInfo.page\", \n",
    "            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n",
    "            \"trafficSource.keyword\", \"trafficSource.medium\", \n",
    "            \"trafficSource.referralPath\", \"trafficSource.source\",\n",
    "            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\n",
    "for col in cat_cols:\n",
    "    print(col)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n",
    "    \n",
    "    \n",
    "num_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \n",
    "for col in num_cols:\n",
    "    train_df[col] = train_df[col].astype(float)\n",
    "    test_df[col] = test_df[col].astype(float)\n",
    "\n",
    "# Split the train dataset into development and valid based on time \n",
    "dev_df = train_df[train_df['date']<=datetime.date(2017,5,31)]\n",
    "val_df = train_df[train_df['date']>datetime.date(2017,5,31)]\n",
    "dev_y = np.log1p(dev_df[\"totals.transactionRevenue\"].values)\n",
    "val_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n",
    "\n",
    "dev_X = dev_df[cat_cols + num_cols] \n",
    "val_X = val_df[cat_cols + num_cols] \n",
    "test_X = test_df[cat_cols + num_cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebdefa0d85540f5bc5a2b9b9ead9dcb8de4b2b72"
   },
   "outputs": [],
   "source": [
    "# custom function to run light gbm model\n",
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"num_leaves\" : 30,\n",
    "        \"min_child_samples\" : 100,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, pred_val_y\n",
    "\n",
    "# Training the model #\n",
    "pred_test, model, pred_val = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69e20bc8b29fd47fccce20da6beb58b64abc0034"
   },
   "source": [
    "Now let us compute the evaluation metric on the validation data as mentioned in this new discussion thread. So we need to do a sum for all the transactions of the user and then do a log transformation on top. Let us also make the values less than 0 to 0 as transaction revenue can only be 0 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "447bbf0b31f2eafa77e8194748995526b46bb7cb"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "pred_val[pred_val<0] = 0\n",
    "val_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n",
    "val_pred_df[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\n",
    "val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "#print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))\n",
    "val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ef81979bdb3a5cbd383ae8a9c26c6c4264af096"
   },
   "source": [
    "So we are getting a validation score of 1.70 using this method against the public leaderboard score of 1.44. So please be cautious while dealing with this.\n",
    "\n",
    "Now let us prepare the submission file similar to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "810fc9736cc40cb1ad09ab62c6aee56085b07d17"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\n",
    "pred_test[pred_test<0] = 0\n",
    "sub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\n",
    "sub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\n",
    "sub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "sub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\n",
    "sub_df.to_csv(\"baseline_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8671d017271d663e8cf55cc830dc990ae970267a"
   },
   "outputs": [],
   "source": [
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
